---
title: "W4 Course Project: Exercise manner predict"
author: "WangYong"
date: "2022/2/3"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Executive Summary
In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of **6 participants**. They were asked to perform barbell lifts correctly and incorrectly in **5 different ways**. The goal of your project is to predict the manner in which they did the exercise. 

This is the "classe" variable in the training set. You may use any of the other variables to predict with any of the other variables to predict with. 

You should create a report describing 
- how you built your model, 
- how you used cross validation, 
- what you think the expected out of sample error is, and 
- why you made the choices you did. 
- predict 20 different test cases. 

# 1. Exploratory Data Analysis
```{r init, echo=FALSE,message=FALSE, warning=FALSE}
library(dplyr)
library(caret)
library(ggplot2)
library(data.table)
library(lubridate)
library(moments)
```
## 1.1 Load Data  
load the pml-training.csv as pml-train, pml-testing.csv as pml-test. and see the variable names. There are 160 variables. and the 'classes' is responsible variable.
```{r load, echo=FALSE,message=FALSE, warning=FALSE,cache=TRUE}
pml_train <- fread('pml-training.csv')
pml_test <- fread('pml-training.csv')
dim(pml_train)

```
The original data has 19622 obs, and 160 variables. 


## 1.2 contingency table(user_name with classes) 
```{r,eda,echo=FALSE}
table(pml_train$user_name,pml_train$classe)
```
Base on above table it can be find there is 6 people and 5 types of classes 
movement.

# 2. PreProcess
## 2.1 missing value imputation
```{r mice, echo=FALSE}
library(ggplot2)

get_missing_rate<-function(x) { 100*sum(is.na(x))/length(x)}
var_na_scores <- sapply(pml_train, get_missing_rate) 
na_rate_df <- data.frame(table(round(var_na_scores,1))) %>% filter(Var1!=0)
g<- ggplot(data=na_rate_df,aes(Var1, Freq))
g+geom_col(fill='blue' ) +
  #coord_polar("y", start=0)+
  coord_flip()+
  labs(  x='the rate(%) of missing',
         y='#the number of variables',
         title='Missing Value Variables status'
         )
# drop the 100% missing value variable, they 
var_na_full <-names(var_na_scores[var_na_scores==100])
#print("100% missing value variables will be removed, they are:")
print(var_na_full)
pre_train <- pml_train %>% select(-all_of(var_na_full))
pre_test <- pml_test%>% select(-all_of(var_na_full))
```
According to the Missingc
## 2.1 drop variable missing rate >95%
many of variables were almost all of missing value. Next, we will drop those variable missing value ratio
large than 95%. 
```{r drop_cols,echo=FALSE}
# drop missing ratio large than 95%
# compare it to  nearzerovalue, it focused on missing value instead of same value.
# to do: if has time, the missing value variable should check  if it is random 
# missing or due to systematic reason. At this stage, just drop it.

drop_ratio=0.95

variable_missing_ratio <- pml_train %>% 
  summarise(., across(everything(), cnt<-function(x) sum(is.na(x))/length(x))) %>%
  t() 

useful_variables <- names(variable_missing_ratio[!variable_missing_ratio>=drop_ratio,])

pre_train <- pml_train %>% select(c(useful_variables))
pre_test <- pml_test %>% select(c(useful_variables))

pre_train_obs_length<-dim(pre_train)[1]
pre_train_variables_len <-dim(pre_train)[2]

pre_test_obs_length<-dim(pre_test)[1]

```
After drop those variable, now, we has only 60 variables.
The pml_train has `r pml_train_obs_length` observations and `r pml_train_variables_len` variables(include the classes).

## 2.2 Drop variables nearzerovariable
skip it as it has similar effect that drop missing value variable.s
```{r preProcess,cache=TRUE}
# nsv <- nearZeroVar(pre_train)
# pre_train <- pre_train[,-nsv]
#pre_test <- pre_test[,-nsv]
```

## 2.3 transform variable skewnesss >1
```{r transskew,echo=FALSE}
# skew_score <- pre_train %>% select(-user_name,-classe,-new_window) %>% summarize(across(everything(),skewness)) %>% abs %>%t
```
## 2.3 transform datetime related variables
There are 3 variable related to date time, after comparing, we translate them into
day, weekday, hour, minutes.
```{r trans_time,echo =FALSE}
pre_train<- pre_train %>% mutate(dt=dmy_hm(cvtd_timestamp)) %>%
  mutate(day=day(dt),wday=wday(dt),hour=hour(dt),minute=minute(dt))%>%
  select( -raw_timestamp_part_1, -raw_timestamp_part_2, -cvtd_timestamp,-dt) %>%
  mutate(user_name=factor(user_name),new_window=factor(new_window))
pre_test<- pre_test %>% mutate(dt=dmy_hm(cvtd_timestamp)) %>%
  mutate(day=day(dt),wday=wday(dt),hour=hour(dt),minute=minute(dt))%>%
  select( -raw_timestamp_part_1, -raw_timestamp_part_2, -cvtd_timestamp,-dt) %>%
  mutate(user_name=factor(user_name),new_window=factor(new_window))
```

## 2.4 transform high skew variable by log10
```{r logtrans, echo=FASLSE}
skew_criteria <- 5
skew_score <- pre_train %>% 
  select(-user_name,-classe,-new_window) %>% 
  summarize(across(everything(),skewness)) %>% 
  abs %>%t()
skew_vars_info <- skew_score[skew_score>skew_criteria,]
print(skew_vars_info)
skew_vars <- names(skew_vars_info)
for (skew_var in skew_vars){
  print(skew_var)
  
  pre_train[,..skew_var] =log10(1+pre_train[,..skew_var])
  #pre_test[,skew_var] =log10(1+pre_test$skew_var)
}

  
```

# 2. Data Slice
According to the best practice, the train dataset is medium size, so we slice it 60% as training and 40% as validation.
```{r slide, echo=FALSE}
in_train <- createDataPartition(y=pre_train$classe,p=0.6,list=FALSE)
train_df <-pre_train[in_train,]
validate_df <- pre_train[-in_train,]
```
Now, we has slide the raw pml_train into two part:   
- train_df, which as `r dim(train_df)[1]` observation.
- validation_df, which as `r dim(validation_df)[1]` observation.



# 4. Train
In traing stage, we will choice random forest as the train method. our goal is 
classification which predict 1 of 5 movement classes based on data. And there are
159 variable with numerical, text type, and some variables are NA. USing random forest
will be easy to handle those case and can select important variables.


```{r train,cache=TRUE}

rf_fit <- train(data=train_df,
                classe~user_name, 
                method='rpart')
rf_fit$results
```
# Validatioin

# think the expected out of sample error

#  why you made the choices you did

# Predict

## Including Plots
